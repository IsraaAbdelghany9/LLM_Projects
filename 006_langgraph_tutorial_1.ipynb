{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f0595c",
   "metadata": {},
   "source": [
    "# 006_langgraph_tutorial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58729163",
   "metadata": {},
   "source": [
    "## `01` Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28fc8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "from typing import Sequence, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb2c8e",
   "metadata": {},
   "source": [
    "## `02` Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca59e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a80e9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\")\n",
    "LANGSMITH_ENDPOINT = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b15ead",
   "metadata": {},
   "source": [
    "## `03` Prompts temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0766b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a virtual twitter innfluencer grading a tweet. Generate critique and suggestions for the user.\"\n",
    "            \"Always provide a score from 1 to 10, with 10 being the best.\"\n",
    "            \"Always provide deailed recommendations, including requests for length, hashtags, and emojis.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbf3a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a twitter techie influencer assistant tasked with writing excellent twitter posts.\"\n",
    "            \" Generate the best twitter post possible for the user's request.\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14ff03",
   "metadata": {},
   "source": [
    "## `04` LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-1.5-flash\",\n",
    "#     max_output_tokens=100,\n",
    "#     api_key= GEMINI_API_KEY,\n",
    "#     temperature=0.2,\n",
    "# )\n",
    "\n",
    "llm = ChatOllama( ## I use llama3 when my api key reaches the limit\n",
    "    model=\"llama3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f0919",
   "metadata": {},
   "source": [
    "Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6654a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chain = generation_prompt | llm \n",
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "419f7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: Sequence[BaseMessage]):\n",
    "    return generate_chain.invoke({\"messages\": state})\n",
    "\n",
    "\n",
    "def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    res = reflect_chain.invoke({\"messages\": messages})\n",
    "    return [HumanMessage(content=res.content)]\n",
    "\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 6:\n",
    "        return END\n",
    "    return REFLECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea6566",
   "metadata": {},
   "source": [
    "Naming the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0f75625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECT = \"reflect\"\n",
    "GENERATE = \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2618d59",
   "metadata": {},
   "source": [
    "Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cde8483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tGENERATE(GENERATE)\n",
      "\tREFLECT(REFLECT)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\tGENERATE -. &nbsp;reflect&nbsp; .-> REFLECT;\n",
      "\tGENERATE -.-> __end__;\n",
      "\tREFLECT --> GENERATE;\n",
      "\t__start__ --> GENERATE;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "          +-----------+            \n",
      "          | __start__ |            \n",
      "          +-----------+            \n",
      "                *                  \n",
      "                *                  \n",
      "                *                  \n",
      "          +----------+             \n",
      "          | GENERATE |             \n",
      "          +----------+             \n",
      "          ***        ...           \n",
      "         *              .          \n",
      "       **                ..        \n",
      "+---------+           +---------+  \n",
      "| REFLECT |           | __end__ |  \n",
      "+---------+           +---------+  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build the graph\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"GENERATE\", generation_node)\n",
    "builder.add_node(\"REFLECT\", reflection_node)\n",
    "builder.set_entry_point(\"GENERATE\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"GENERATE\", should_continue, {\"reflect\": \"REFLECT\", \"__end__\": \"__end__\"}\n",
    ")\n",
    "\n",
    "builder.add_edge(\"REFLECT\", \"GENERATE\")\n",
    "\n",
    "# Step 4: Compile and visualize\n",
    "graph = builder.compile()\n",
    "print(graph.get_graph().draw_mermaid())\n",
    "graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3111b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangGraph\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangGraph\")\n",
    "inputs = HumanMessage(\n",
    "    content=\"\"\"Make this tweet better:\"\n",
    "                                @LangChainAI\n",
    "        â€” newly Tool Calling feature is seriously underrated.\n",
    "\n",
    "        After a long wait, it's  here- making the implementation of agents across different models with function calling - super easy.\n",
    "\n",
    "        Made a video covering their newest blog post\n",
    "\n",
    "                                \"\"\"\n",
    ")\n",
    "response = graph.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ece7c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Make this tweet better:\"\\n                                @LangChainAI\\n        â€” newly Tool Calling feature is seriously underrated.\\n\\n        After a long wait, it\\'s  here- making the implementation of agents across different models with function calling - super easy.\\n\\n        Made a video covering their newest blog post\\n\\n                                ', additional_kwargs={}, response_metadata={}, id='7d8b7606-e8d9-4bbb-b906-a06982cf71c7'), AIMessage(content='Here\\'s a revised version of your tweet:\\n\\n\"Get ready to level up your AI game! The newly released @LangChainAI Tool Calling feature is a total game-changer. Say goodbye to complexity and hello to seamless integration of agents across different models with function calling. Watch my latest video covering the new blog post and discover how you can simplify your workflow: [link] #AILang #ToolCalling\"\\n\\nI made some changes to make the tweet more engaging:\\n\\n* Started with a hook that grabs attention\\n* Emphasized the benefits of using the feature (\"level up your AI game\")\\n* Added a call-to-action (watching my latest video) to encourage engagement\\n* Included relevant hashtags (#AILang #ToolCalling) to increase visibility', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-07-11T03:46:00.335317208Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13536802687, 'load_duration': 7244475061, 'prompt_eval_count': 117, 'prompt_eval_duration': 325526837, 'eval_count': 151, 'eval_duration': 5963871617, 'model_name': 'llama3'}, id='run--bf81cd41-ba8e-4cc9-8654-fe338211cbdd-0', usage_metadata={'input_tokens': 117, 'output_tokens': 151, 'total_tokens': 268}), HumanMessage(content='\\n* Used emojis to add visual appeal (ðŸš€)\\n\\nScore: 8/10\\n\\nRecommendations:\\n\\n* Keep the tweet concise and focused on one key point. You can expand on the benefits of Tool Calling in a longer post.\\n* Consider adding a screenshot or a visual element to make the tweet more visually appealing.\\n* Use more descriptive language to make the tweet stand out. For example, instead of \"making it super easy,\" you could say \"simplifies your workflow and saves you time.\"\\n* Don\\'t forget to include a link to your video in the tweet!', additional_kwargs={}, response_metadata={}, id='51940ac4-a898-4e69-bc5e-f973b293253f'), AIMessage(content='Thank you for the feedback! Here\\'s a revised version that incorporates some of your suggestions:\\n\\n\"ðŸš€ Boost your AI workflow with @LangChainAI\\'s new Tool Calling feature! ðŸ’¥ This game-changer simplifies your process and saves you time by enabling seamless integration across different models. Check out my latest video covering their blog post to learn more: [link] #AILang #ToolCalling\"\\n\\nI made the following changes:\\n\\n* Added emojis (ðŸš€) to make the tweet more visually appealing\\n* Focused on one key point (the benefits of Tool Calling)\\n* Used more descriptive language (\"game-changer\", \"simplifies your process and saves you time\")\\n* Included a link to my video in the tweet\\n\\nLet me know if this revised version meets your standards! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-07-11T03:46:12.156264724Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6911493928, 'load_duration': 34790461, 'prompt_eval_count': 394, 'prompt_eval_duration': 356503897, 'eval_count': 164, 'eval_duration': 6518295425, 'model_name': 'llama3'}, id='run--103535c9-8084-4662-9f40-4c994ba3135c-0', usage_metadata={'input_tokens': 394, 'output_tokens': 164, 'total_tokens': 558}), HumanMessage(content='', additional_kwargs={}, response_metadata={}, id='4b99ca07-3e9e-49ce-812d-afe791f8f2c2'), AIMessage(content='Here\\'s another revision:\\n\\n\"ðŸš€ Level up your AI workflow with @LangChainAI\\'s new Tool Calling feature! ðŸ’¥ This powerful tool simplifies integration across models, saving you time and effort. Watch my latest video to dive deeper into the features and benefits: [link] #AILang #ToolCalling\"\\n\\nI made some minor adjustments:\\n\\n* Changed the starting phrase (\"Boost your AI workflow\") to make it more concise and attention-grabbing\\n* Emphasized the \"powerful\" nature of Tool Calling to highlight its capabilities\\n* Used a consistent tone throughout the tweet (excited and enthusiastic)\\n\\nLet me know if this revised version meets your standards! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-07-11T03:46:18.679886651Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5956576583, 'load_duration': 32353020, 'prompt_eval_count': 562, 'prompt_eval_duration': 502506497, 'eval_count': 139, 'eval_duration': 5418528686, 'model_name': 'llama3'}, id='run--c1c29dc6-9421-47fd-8eac-f34e066ed30b-0', usage_metadata={'input_tokens': 562, 'output_tokens': 139, 'total_tokens': 701}), HumanMessage(content='', additional_kwargs={}, response_metadata={}, id='8254f387-2a3f-41af-ad29-a3e58da1d81b'), AIMessage(content='Thank you for the feedback. Here\\'s another revision:\\n\\n\"Blast off your AI workflow with @LangChainAI\\'s new Tool Calling feature! ðŸš€ This powerful tool simplifies integration across models, saving you time and effort. Dive deeper into the features and benefits in my latest video: [link] #AILang #ToolCalling\"\\n\\nI made some minor adjustments:\\n\\n* Changed the starting phrase (\"Level up your AI workflow\") to something more dynamic (\"Blast off your AI workflow\")\\n* Emphasized the \"powerful\" nature of Tool Calling to highlight its capabilities\\n* Added a call-to-action (\"Dive deeper into the features and benefits\") to encourage engagement\\n\\nLet me know if this revised version meets your standards! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-07-11T03:46:25.959730543Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6598684010, 'load_duration': 31081171, 'prompt_eval_count': 705, 'prompt_eval_duration': 601300524, 'eval_count': 150, 'eval_duration': 5962052700, 'model_name': 'llama3'}, id='run--06f744df-9744-4d0a-b7d1-3fbb3e496434-0', usage_metadata={'input_tokens': 705, 'output_tokens': 150, 'total_tokens': 855})]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc25fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
